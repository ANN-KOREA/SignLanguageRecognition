{"class_name": "Tokenizer", "config": {"num_words": null, "filters": "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n", "lower": true, "split": " ", "char_level": false, "oov_token": null, "document_count": 1, "word_counts": "{\"computer\": 1, \"deutschland\": 1, \"hallo\": 1, \"welt\": 1}", "word_docs": "{\"hallo\": 1, \"computer\": 1, \"deutschland\": 1, \"welt\": 1}", "index_docs": "{\"3\": 1, \"1\": 1, \"2\": 1, \"4\": 1}", "index_word": "{\"1\": \"computer\", \"2\": \"deutschland\", \"3\": \"hallo\", \"4\": \"welt\"}", "word_index": "{\"computer\": 1, \"deutschland\": 2, \"hallo\": 3, \"welt\": 4}"}}