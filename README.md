# SignLanguageRecognition

## Description

This repository contains a variety of tools to build up a system for recognizing signs of the german sign language (DGS).
The idea is to provide a live translation in a webcam stream. This could be on mobile or desktop.
For this we train a deep learning model (RNN) for predicting the actual signs made by a person filmed.
Therefore we use [MediaPipe](https://github.com/google/mediapipe), a framework for building ML pipelines, to extract face and hand positions, including multiple coordinates for each finger.
Our workflow is described below.

## Workflow

### 1. Gathering video exampels

For training we need many videos for each sign, we want to predict. Those examples are generated by users of our platform [Geb√§rdenfutter](https://gebaerdenfutter.de).

### 2. Extracting face and hand positions

For extracting multi hand and face detections for each frame of the videos and saving them, we built a pipeline with `MediaPipe`, e.g. have a look at the `DetectionsToCSVCalculator`, we implemented. It simply writes out the detections made by `MediaPipe` to CSV files.

## 2. Training deep learning model

The CSV files are used to train a deep learning model with `Keras`, a high level API for `TensorFlow`.
Therefore we use jupyter notebooks to simply write and comment scripts.
Check out the folder `lab`.

## 3. Live prediction

The trained model is used for predicting live video stream.
Therefore another `MediaPipe` pipeline was build. 
`To be continued`
